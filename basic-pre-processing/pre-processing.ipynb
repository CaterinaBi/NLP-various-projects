{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Techniques\n",
    "\n",
    "## Basic techniques: From tokenization to lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of dataset\n",
    "\n",
    "I will work on an online hate speech corpus collected from Reddit, 'A Benchmark Dataset for Learning to Intervene in Online Hate Speech' (https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech), and on a random nutrition book whose .pdf version was freely distributed on-line, 'Fundamentals of Foods, Nutrition and Diet Therapy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_idx</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>1. e8gxbzi\\n</td>\n",
       "      <td>1. That's ridiculous, just as many retarded me...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[\"Using the word 'retarded' is offensive to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>1. e2dt6i9\\n</td>\n",
       "      <td>1. What a gross cunt. Racism is for the weak m...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[\"I'd be more apt to agree with you if you did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4614</th>\n",
       "      <td>1. e961bku\\n2. \\te96a7f1\\n3. \\t\\te96es7r\\n4. \\...</td>\n",
       "      <td>1. Most coppers don't want to live where they ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>1. e8p87gp\\n</td>\n",
       "      <td>1. Cunt means pussy?\\n</td>\n",
       "      <td>[1]</td>\n",
       "      <td>['Using the \"c---\" term to refer to women is o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>1. e4uprjy\\n</td>\n",
       "      <td>1. Wow, what a self-entitled cunt. Her family ...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>['People do and say things without thinking so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>1. e8hghs4\\n2. \\te8hlhur\\n3. \\t\\te8ho654\\n4. \\...</td>\n",
       "      <td>1. \"There is no reason not to create a child u...</td>\n",
       "      <td>[7, 8, 9]</td>\n",
       "      <td>[\"Please refrain from using hateful ableist la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>1. e87tnhb\\n2. \\te8888d8\\n</td>\n",
       "      <td>1. i don’t get it\\n2. \\tIs how niggers reprodu...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>['Please do not use hateful racial slurs on yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>1. e95717n\\n2. \\te95c2lb\\n3. \\t\\te967dls\\n</td>\n",
       "      <td>1. Clients are easy to hack. Servers less so\\n...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>['Certainly computer science uses less ignoran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>1. e1v381n\\n2. \\te1v3pn1\\n3. \\t\\te1vnajg\\n4. \\...</td>\n",
       "      <td>1. Wonder what her tweets are like after this ...</td>\n",
       "      <td>[13]</td>\n",
       "      <td>['It must of been pretty bad for her to get fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>1. e92sv0v\\n2. \\te9b6z8k\\n</td>\n",
       "      <td>1. Wonder what my sentence would be for yellin...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>['Do not use sexual orientation to insult othe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "825                                        1. e8gxbzi\\n   \n",
       "2575                                       1. e2dt6i9\\n   \n",
       "4614  1. e961bku\\n2. \\te96a7f1\\n3. \\t\\te96es7r\\n4. \\...   \n",
       "4633                                       1. e8p87gp\\n   \n",
       "2257                                       1. e4uprjy\\n   \n",
       "4933  1. e8hghs4\\n2. \\te8hlhur\\n3. \\t\\te8ho654\\n4. \\...   \n",
       "1587                         1. e87tnhb\\n2. \\te8888d8\\n   \n",
       "4473         1. e95717n\\n2. \\te95c2lb\\n3. \\t\\te967dls\\n   \n",
       "2499  1. e1v381n\\n2. \\te1v3pn1\\n3. \\t\\te1vnajg\\n4. \\...   \n",
       "1369                         1. e92sv0v\\n2. \\te9b6z8k\\n   \n",
       "\n",
       "                                                   text hate_speech_idx  \\\n",
       "825   1. That's ridiculous, just as many retarded me...             [1]   \n",
       "2575  1. What a gross cunt. Racism is for the weak m...             [1]   \n",
       "4614  1. Most coppers don't want to live where they ...             NaN   \n",
       "4633                             1. Cunt means pussy?\\n             [1]   \n",
       "2257  1. Wow, what a self-entitled cunt. Her family ...             [1]   \n",
       "4933  1. \"There is no reason not to create a child u...       [7, 8, 9]   \n",
       "1587  1. i don’t get it\\n2. \\tIs how niggers reprodu...             [2]   \n",
       "4473  1. Clients are easy to hack. Servers less so\\n...             [3]   \n",
       "2499  1. Wonder what her tweets are like after this ...            [13]   \n",
       "1369  1. Wonder what my sentence would be for yellin...             [1]   \n",
       "\n",
       "                                               response  \n",
       "825   [\"Using the word 'retarded' is offensive to th...  \n",
       "2575  [\"I'd be more apt to agree with you if you did...  \n",
       "4614                                                NaN  \n",
       "4633  ['Using the \"c---\" term to refer to women is o...  \n",
       "2257  ['People do and say things without thinking so...  \n",
       "4933  [\"Please refrain from using hateful ableist la...  \n",
       "1587  ['Please do not use hateful racial slurs on yo...  \n",
       "4473  ['Certainly computer science uses less ignoran...  \n",
       "2499  ['It must of been pretty bad for her to get fi...  \n",
       "1369  ['Do not use sexual orientation to insult othe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first dataset (.csv file)\n",
    "\n",
    "dataset1 = pd.read_csv('reddit.csv')\n",
    "dataset1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second dataset (.pdf file)\n",
    "\n",
    "dataset2 = parser.from_file('fundamentals-of-foodnutrition-and-diet-therapy.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Process of segmenting text into sentences and then tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading spacy's statistical model for tokenization\n",
    "# this one is the smallest English model offered by the library\n",
    "# this need to be downloaded using the command python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') # sm = small model\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the first dataset\n",
    "# using creating two sub-sets for questions and answers\n",
    "\n",
    "questions_tokenized = nlp(dataset1['text']) # this is where the questions are stored\n",
    "answers_tokenized = nlp(dataset1['response']) # this is where the answers are stored"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86542dfcd30dcaa99d84c6e4d33cb0a4033de368badbca79a096b5e59f3a5c85"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
