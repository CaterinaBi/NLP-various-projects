{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Techniques\n",
    "\n",
    "## Basic techniques: From tokenization to lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of dataset\n",
    "\n",
    "I will work on an online hate speech corpus collected from Reddit, 'A Benchmark Dataset for Learning to Intervene in Online Hate Speech' (https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech), and on a random nutrition book whose .pdf version was freely distributed on-line, 'Fundamentals of Foods, Nutrition and Diet Therapy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_idx</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1. e8alpdr\\n2. \\te8ano1t\\n3. \\t\\te8ap5ux\\n4. \\...</td>\n",
       "      <td>1. /r/forwardsfromgrandpa   jesus.  1, the fir...</td>\n",
       "      <td>[6, 13]</td>\n",
       "      <td>['Its inappropriate to use words that attack o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>1. e2mdroc\\n</td>\n",
       "      <td>1. Aw boohoo and bullshit. My hand was recentl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>1. e9ccslq\\n2. \\te9cg5t7\\n3. \\t\\te9crfs5\\n4. \\...</td>\n",
       "      <td>1. Is it weird all these rogue employee action...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>1. e82osko\\n2. \\te82t024\\n3. \\t\\te82z8u1\\n4. \\...</td>\n",
       "      <td>1. I would be willing to bet Crunchyroll was a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>1. e5xuncu\\n</td>\n",
       "      <td>1. But mysandry is okay, right?  Fucking retar...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>['Try the word moron instead, easy.', 'As much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>1. e9d0szh\\n</td>\n",
       "      <td>1. That’s spicy!\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>1. e92386q\\n2. \\te923g6k\\n</td>\n",
       "      <td>1. They claim feminism is the future but no fe...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>['Using the c-word here hurts and demeans wome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1. e8tjcb5\\n</td>\n",
       "      <td>1. Spook day and I'm stuck at a doctors office...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>1. e9bezmk\\n2. \\te9bfuyz\\n3. \\t\\te9chg71\\n</td>\n",
       "      <td>1. This is the problem with these kinds of fil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>1. e5noxtf\\n</td>\n",
       "      <td>1. Vile, manipulative, and entitled. What a de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "1351  1. e8alpdr\\n2. \\te8ano1t\\n3. \\t\\te8ap5ux\\n4. \\...   \n",
       "3976                                       1. e2mdroc\\n   \n",
       "1251  1. e9ccslq\\n2. \\te9cg5t7\\n3. \\t\\te9crfs5\\n4. \\...   \n",
       "1645  1. e82osko\\n2. \\te82t024\\n3. \\t\\te82z8u1\\n4. \\...   \n",
       "4551                                       1. e5xuncu\\n   \n",
       "2764                                       1. e9d0szh\\n   \n",
       "3865                         1. e92386q\\n2. \\te923g6k\\n   \n",
       "872                                        1. e8tjcb5\\n   \n",
       "849          1. e9bezmk\\n2. \\te9bfuyz\\n3. \\t\\te9chg71\\n   \n",
       "2663                                       1. e5noxtf\\n   \n",
       "\n",
       "                                                   text hate_speech_idx  \\\n",
       "1351  1. /r/forwardsfromgrandpa   jesus.  1, the fir...         [6, 13]   \n",
       "3976  1. Aw boohoo and bullshit. My hand was recentl...             NaN   \n",
       "1251  1. Is it weird all these rogue employee action...             NaN   \n",
       "1645  1. I would be willing to bet Crunchyroll was a...             NaN   \n",
       "4551  1. But mysandry is okay, right?  Fucking retar...             [1]   \n",
       "2764                                 1. That’s spicy!\\n             NaN   \n",
       "3865  1. They claim feminism is the future but no fe...             [1]   \n",
       "872   1. Spook day and I'm stuck at a doctors office...             NaN   \n",
       "849   1. This is the problem with these kinds of fil...             NaN   \n",
       "2663  1. Vile, manipulative, and entitled. What a de...             NaN   \n",
       "\n",
       "                                               response  \n",
       "1351  ['Its inappropriate to use words that attack o...  \n",
       "3976                                                NaN  \n",
       "1251                                                NaN  \n",
       "1645                                                NaN  \n",
       "4551  ['Try the word moron instead, easy.', 'As much...  \n",
       "2764                                                NaN  \n",
       "3865  ['Using the c-word here hurts and demeans wome...  \n",
       "872                                                 NaN  \n",
       "849                                                 NaN  \n",
       "2663                                                NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first dataset (.csv file)\n",
    "\n",
    "dataset1 = pd.read_csv('reddit.csv')\n",
    "dataset1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5020, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second dataset (.pdf file)\n",
    "\n",
    "dataset2 = parser.from_file('fundamentals-of-foodnutrition-and-diet-therapy.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Process of segmenting text into sentences and then tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading spacy's statistical model for tokenization\n",
    "# this one is the smallest English model offered by the library\n",
    "# this need to be downloaded using the command python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') # sm = small model\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tokenizing the first dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# using creating two sub-sets for questions and answers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the results are Doc container objects\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m questions_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# this is where the questions are stored\u001b[39;00m\n\u001b[1;32m      6\u001b[0m answers_tokenized \u001b[38;5;241m=\u001b[39m nlp(dataset1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py:1014\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=992'>993</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=993'>994</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=994'>995</a>\u001b[0m     text: Union[\u001b[39mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=997'>998</a>\u001b[0m     component_cfg: Optional[Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=998'>999</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=999'>1000</a>\u001b[0m     \u001b[39m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1000'>1001</a>\u001b[0m \u001b[39m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1001'>1002</a>\u001b[0m \u001b[39m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1011'>1012</a>\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1012'>1013</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1013'>1014</a>\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text)\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1014'>1015</a>\u001b[0m     \u001b[39mif\u001b[39;00m component_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1015'>1016</a>\u001b[0m         component_cfg \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py:1108\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1105'>1106</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1106'>1107</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\u001b[39m.\u001b[39mfrom_bytes(doc_like)\n\u001b[0;32m-> <a href='file:///Users/caterinabonan/miniforge3/envs/NLP/lib/python3.10/site-packages/spacy/language.py?line=1107'>1108</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE1041\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(doc_like)))\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "# tokenizing the first dataset\n",
    "# using creating two sub-sets for questions and answers\n",
    "# the results are Doc container objects\n",
    "\n",
    "questions_tokenized = nlp(dataset1['text']) # this is where the questions are stored\n",
    "answers_tokenized = nlp(dataset1['response']) # this is where the answers are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'questions_tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mquestions_tokenized\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'questions_tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "print(questions_tokenized)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86542dfcd30dcaa99d84c6e4d33cb0a4033de368badbca79a096b5e59f3a5c85"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
