{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic pre-processing of textual dataset\n",
    "# For EDA and language model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook demonstrates basic pre-processing techniques using the `nltk` and `spaCy` libraries\n",
    "\n",
    "The task is done to prepare datasets of hate speech for use in the Omdena Aswan Local Chapter, 'Detecting Hateful and Offensive Language using NLP', in which I am cooperating and co-leading the pre-processing and EDA tasks with Vishu Kalier.\n",
    "\n",
    "Here, we will be using the convabuse (https://github.com/amandacurry/convabuse) dataset.\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "I shall complete the task by strictly following the guidelines outlined for the project. These are as follows.\n",
    "\n",
    "### File structure\n",
    "\n",
    "The output of the pre-processing has to be a `.cvs` file. The file has to be organised in 5 columns as follows: \n",
    "| corpus_name | raw_sentence | label | clean_sentence_training | clean_sentence_EDA | \n",
    "\n",
    "### Labels\n",
    "\n",
    "The labels will be:\n",
    "\n",
    "- '2' for RISKY sentences (eg., 'hateful' or 'abusive');\n",
    "- '1' for POTENTIALLY RISKY sentences (e.g., 'offensive)';\n",
    "- '0' for NON RISKY sentences.\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "The pre-processing for the 'clean_sentence_training' column involves the following steps:\n",
    "\n",
    "1- delete all rows containing null values;\n",
    "\n",
    "2- stopwords removal;\n",
    "\n",
    "3- removal of digits and words containing digits, punctuation and special characters, extra spaces, links;\n",
    "\n",
    "4- lemmatization (NO stemming at it can yield undesirable results).\n",
    "\n",
    "The case of the sentences shoudn't be changed as the distinction between lowercase and uppercase can be meaningful in this context. The pre-processing for the 'clean_sentence_EDA' involves all the steps above, plus the deletion of all emojis (if present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/caterinabonan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "# don't forget to install any library that you don't have in your workspace yet\n",
    "# make sure that the environment you're installing your libraries corresponds to the one you're using to compile your code\n",
    "# note that the downloads form nltk might take a while\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86542dfcd30dcaa99d84c6e4d33cb0a4033de368badbca79a096b5e59f3a5c85"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
